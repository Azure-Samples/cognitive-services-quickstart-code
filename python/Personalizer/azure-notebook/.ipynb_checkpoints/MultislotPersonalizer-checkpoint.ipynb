{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Services Personalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial simulates a Multislot Personalizer loop _system_ which suggests which products a customer should buy when displayed in different slots. The users and their preferences are store in a [user dataset](simulated_users.json). Information about the products is also available in a [product dataset](products.json). Information about the slots is also available in a [slot dataset](slots.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the user information is:\n",
    "\n",
    "|Customers|Days of month|Times of Day|Types of Device|\n",
    "|--|--|--|--|\n",
    "|Alice<br>Bob<br>Cathy|7<br>21<br>30|Morning<br>Afternoon<br>Evening|Desktop<br>Mobile|\n",
    "\n",
    "To help Personalizer make the correct product selection for each person, the _system_ also knows details about the product.\n",
    "\n",
    "|Product on Sale|Price|Category|\n",
    "|--|--|--|\n",
    "|true<br>false|20<br>70<br>200<br>499|Clothing<br>Sports<br>Electronics|\n",
    "\n",
    "The _system_ also knows details about the slots where the recommended products will be displayed.\n",
    "\n",
    "|Size|Position|\n",
    "|--|--|\n",
    "|Large<br>Small|Left<br>Right|\n",
    "\n",
    "The **purpose** of the Personalizer loop is to find the best match between the users and the product as much of the time as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the simulation works\n",
    "\n",
    "At the beginning of the running system, the suggestions from Personalizer are only successful between 20% to 30% (indicated by the reward score of 1). After some requests, the system improves.\n",
    "\n",
    "After the initial 25,000 requests, run an offline evaluation. This allows Personalizer to review the data and suggest a better learning policy. Apply the new learning policy and run the notebook again with 5,000 requests. The loop will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multislot Rank and Reward calls\n",
    "For each of the few thousand calls to the Personalizer service, the Azure notebook sends the **Multislot Rank** request to the REST API:\n",
    "* A unique ID for the Multislot Rank/Request event\n",
    "* Context - A random choice of the user, day of month, time of day, and device - simulating a user on a website or mobile device \n",
    "* Features - All the product data - from which Personalizer makes a suggestion \n",
    "* Slots - All the slot information where the recommended products will be displayed\n",
    "\n",
    "The system receives the Multislot rank of the product choices, then compares that prediction with the user's known choice for the same day of month, time of day and device. If the known choice is the same as the prediction choice, the **Reward** of 1 is sent back to Personalizer. Otherwise the reward is 0. \n",
    "\n",
    "> [!Note]\n",
    "> This is a simulation so the algorithm for the reward is simple. In a real-world scenario, the algorithm should use business logic, possibly with weights for various aspects of the customer's experience, to determine the reward score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* An [Azure Personalizer resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesPersonalizer).\n",
    "* If you have already used the Personalizer resource, make sure to [clear the data](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-settings#clear-data-for-your-learning-loop) in the Azure portal for the resource.\n",
    "* Upload all the files for this sample into an Azure Notebook project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File descriptions:\n",
    "\n",
    "* [MultislotPersonalizer.ipynb](MultislotPersonalizer.ipynb) is the Jupyter notebook for this tutorial.\n",
    "* [User dataset](simulated_users.json) is stored in a JSON object.\n",
    "* [Product dataset](products.json) is stored in a JSON object.\n",
    "* [Slot dataset](slots.json) is stored in a JSON object.\n",
    "* [Example Request JSON](example-rankrequest.json) is the expected format for a POST request to the Rank API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Personalizer resource\n",
    "    \n",
    "In the Azure portal, configure your [Personalizer resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesPersonalizer) with the **update model frequency** set to 30 seconds and a **reward wait time** of 30 seconds. These settings are found in the Configurations tab, under the RESOURCE MANAGEMENT section.    \n",
    "\n",
    "|Setting|Value|\n",
    "|--|--|\n",
    "|update model frequency|30 seconds|\n",
    "|reward wait time|30 seconds|    \n",
    "\n",
    "These values have a very short duration in order to show changes in this tutorial. These values shouldn't be used in a production scenario without validating they achieve your goal with your Personalizer loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrade Personalizer instance to Multislot\n",
    "In the Azure portal, in the Personalizer resouce on the **Model and learning settings page**, click **Export learning settings**. The **arguments** field in the downloaded json file will start with **--cb_explore_adf**. Change this to **--ccb_explore_adf** while leaving the rest of the file untouched. Save the file. \n",
    "\n",
    "In the same tab in the portal, under **import learning settings** browse to find your recently modified json file and upload it. This will update your Personalizer instance to be a Multislot Personalizer and will now support Multislot Rank and Reward calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Azure Notebook \n",
    "Change the Kernel to Python 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Notebook cells\n",
    "Run each executable cell and wait for it to return.\n",
    "\n",
    "You know it is done when the brackets next to the cell display a number instead of a `*`. Do not continue if you get an error.\n",
    "\n",
    "The following sections explain what each cell does programmatically and what to expect for the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include the python modules\n",
    "Include the required python modules. The cell has no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Personalizer resource key and name\n",
    "\n",
    "From the Azure portal, find your key and endpoint on the **Quickstart** page of your Personalizer resource. Change the value of `<your-resource-name>` to your Personalizer resource's name. Change the value of `<your-resource-key>` to your Personalizer key. \n",
    "\n",
    "The cell has no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'personalization_base_url' and 'resource_key' with your valid endpoint values.\n",
    "personalization_base_url = \"https://<your-resource-name>.cognitiveservices.azure.com/\"\n",
    "resource_key = \"<your-resource-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print current date and time\n",
    "Use this function to note the start and end times of the iterative function, `iterations`.\n",
    "\n",
    "These cells have no output. The function does output the current date and time when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out current datetime\n",
    "def currentDateTime():\n",
    "    currentDT = datetime.datetime.now()\n",
    "    print (str(currentDT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the last model update date and time\n",
    "\n",
    "When the function, `get_last_updated`, is called, the function prints out the last modified date and time that the model was updated. \n",
    "\n",
    "These cells have no output. The function does output the last model training date when called.\n",
    "\n",
    "The function uses a GET REST API to [get model properties](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/GetModelProperties). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variable for model's last modified date\n",
    "modelLastModified = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_updated(currentModifiedDate):\n",
    "\n",
    "    print('-----checking model')\n",
    "\n",
    "    # get model properties\n",
    "    response = requests.get(personalization_model_properties_url, headers = headers, params = None)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    # get lastModifiedTime\n",
    "    lastModifiedTime = json.dumps(response.json()[\"lastModifiedTime\"])\n",
    "\n",
    "    if (currentModifiedDate != lastModifiedTime):\n",
    "        currentModifiedDate = lastModifiedTime\n",
    "        print(f'-----model updated: {lastModifiedTime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get policy and service configruation\n",
    "\n",
    "Validate the state of the service with these two REST calls.\n",
    "\n",
    "These cells have no output. The function does output the service settings when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_settings():\n",
    "\n",
    "    print('-----checking service settings')\n",
    "\n",
    "    # get learning policy\n",
    "    response = requests.get(personalization_model_policy_url, headers = headers, params = None)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    # get service settings\n",
    "    response = requests.get(personalization_service_configuration_url, headers = headers, params = None)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct URLs for REST calls and read JSON data files\n",
    "\n",
    "This cell \n",
    "\n",
    "* builds the URLs used in REST calls \n",
    "* sets the security header using your Personalizer resource key \n",
    "* sets the random seed for the Rank event ID\n",
    "* reads in the JSON data files\n",
    "* calls `get_last_updated` method - learning policy has been removed in example output\n",
    "* calls `get_service_settings` method\n",
    "\n",
    "The cell has output from the call to `get_last_updated` and `get_service_settings` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build URLs\n",
    "personalization_rank_url = personalization_base_url + \"personalizer/v1.1-preview.1/multislot/rank\"\n",
    "personalization_reward_url = personalization_base_url + \"personalizer/v1.1-preview.1/multislot/events/\" #add \"{eventId}/reward\"\n",
    "personalization_model_properties_url = personalization_base_url + \"personalizer/v1.1-preview.1/model/properties\"\n",
    "personalization_model_policy_url = personalization_base_url + \"personalizer/v1.1-preview.1/configurations/policy\"\n",
    "personalization_service_configuration_url = personalization_base_url + \"personalizer/v1.1-preview.1/configurations/service\"\n",
    "headers = {'Ocp-Apim-Subscription-Key': resource_key, 'Content-Type': 'application/json'}\n",
    "\n",
    "# context\n",
    "users = \"simulated_users.json\"\n",
    "\n",
    "# action features\n",
    "products = \"products.json\"\n",
    "\n",
    "# slot features\n",
    "slots = \"slots.json\"\n",
    "\n",
    "# empty JSON for Rank request\n",
    "requestpath = \"example-rankrequest.json\"\n",
    "\n",
    "# initialize random\n",
    "random.seed(time.time())\n",
    "\n",
    "simulated_users = None\n",
    "rankactionsjsonobj = None\n",
    "actionfeaturesobj = None\n",
    "slotfeaturesobj = None\n",
    "\n",
    "with open(users) as handle:\n",
    "    simulated_users = json.loads(handle.read())\n",
    "\n",
    "with open(products) as handle:\n",
    "    actionfeaturesobj = json.loads(handle.read())\n",
    "\n",
    "with open(slots) as handle:\n",
    "    slotfeaturesobj = json.loads(handle.read())\n",
    "\n",
    "with open(requestpath) as handle:\n",
    "    rankactionsjsonobj = json.loads(handle.read())\n",
    "\n",
    "get_last_updated(modelLastModified)\n",
    "get_service_settings()\n",
    "\n",
    "print(f'Users count {len(simulated_users)}')\n",
    "print(f'Products count {len(actionfeaturesobj)}')\n",
    "print(f'Slots count {len(slotfeaturesobj)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting the first REST call\n",
    "\n",
    "This previous cell is the first cell that calls out to Personalizer. Make sure the REST status code in the output is `<Response [200]>`. If you get an error, such as 404, but you are sure your resource key and name are correct, reload the notebook.\n",
    "\n",
    "Make sure the count of users, products and slots are 3, 4 and 2 respectively. If you get an error, check that you uploaded all 4 JSON files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up metric chart in Azure portal\n",
    "\n",
    "Later in this tutorial, the long running process of 25,000 requests is visible from the browser with an updating text box. It may be easier to see in a chart or as a total sum, when the long running process ends. To view this information, use the metrics provided with the resource. You can create the chart now that you have completed a request to the service, then refresh the chart periodically while the long running process is going.\n",
    "\n",
    "1. In the Azure portal, select your Personalizer resource.\n",
    "1. In the resource navigation, select **Metrics** underneath Monitoring. \n",
    "1. In the chart, select **Add metric**.\n",
    "1. The resource and metric namespace are already set. You only need to select the metric of **successful calls** and the aggregation of **sum**.\n",
    "1. Change the time filter to the last 4 hours.\n",
    "\n",
    "    You should see three successful calls in the chart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a unique event ID\n",
    "This function generates a unique ID for each rank call. The ID is used to identify the rank and reward call information. This value could come from a business process such as a web view ID or transaction ID.\n",
    "\n",
    "The cell has no output. The function does output the unique ID when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_id(rankjsonobj):\n",
    "    eventid = uuid.uuid4().hex\n",
    "    rankjsonobj[\"eventId\"] = eventid\n",
    "    return eventid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random user, day of month, time of day and device \n",
    "\n",
    "This function selects a unique user, day of month, time of day, and device, then adds those items to the JSON object to send to the Multislot Rank request. \n",
    "\n",
    "The cell has no output. When the funciton is called it returns the random user's name, random day of month, random time of day and random device. \n",
    "\n",
    "The list of 3 users and their preferences - only some preferences are shown for brevity:\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"Alice\": {\n",
    "\t\t\"7\": {\n",
    "\t\t\t\"Morning\": {\n",
    "\t\t\t\t\"Desktop\": {\n",
    "\t\t\t\t\t\"BigHeroPosition\": [],\n",
    "\t\t\t\t\t\"SmallSidebar\": [\"Tennis-Racket-133\"]\n",
    "\t\t\t\t}...\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"Bob\": {\n",
    "\t\t\"7\": {\n",
    "\t\t\t\"Morning\": {\n",
    "\t\t\t\t\"Desktop\": {\n",
    "\t\t\t\t\t\"BigHeroPosition\": [],\n",
    "\t\t\t\t\t\"SmallSidebar\": []\n",
    "\t\t\t\t}...\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"Cathy\": {\n",
    "\t\t\"7\": {\n",
    "\t\t\t\"Morning\": {\n",
    "\t\t\t\t\"Desktop\": {\n",
    "\t\t\t\t\t\"BigHeroPosition\": [\"Red-Polo-Shirt-432\"],\n",
    "\t\t\t\t\t\"SmallSidebar\": []\n",
    "\t\t\t\t}...\t\t\t\t\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_user_and_contextfeatures(namesopt, dayofmonthopt, timeofdayopt, deviceopt, rankjsonobj):\n",
    "    name = namesopt[random.randint(0,2)]\n",
    "    dayofmonth = dayofmonthopt[random.randint(0,2)]\n",
    "    timeofday = timeofdayopt[random.randint(0,2)]\n",
    "    device = deviceopt[random.randint(0,1)]\n",
    "    rankjsonobj['contextFeatures'] = [{'name': name, 'dayofmonth': dayofmonth, 'timeofday': timeofday, 'device': device}]\n",
    "    return [name, dayofmonth, timeofday, device]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all product data \n",
    "\n",
    "This functions adds the entire list of product to the JSON object to send to the Multislot Rank request. \n",
    "\n",
    "The cell has no output. The function does change the rankjsonobj when called.\n",
    "\n",
    "The example of a single product's features is:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"Red-Polo-Shirt-432\",\n",
    "\t\"features\": [{\n",
    "\t\t\"onSale\": true,\n",
    "\t\t\"price\": 20,\n",
    "\t\t\"category\": \"Clothing\"\n",
    "\t}]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_action_features(rankjsonobj):\n",
    "    rankjsonobj['actions'] = actionfeaturesobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all slot data \n",
    "\n",
    "This function adds the entire list of slot to the JSON object to send to the Multislot Rank request.\n",
    "\n",
    "The cell has no output. The function does change the rankjsonobj when called. \n",
    "\n",
    "The example of a single slot's feature is:\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"id\": \"BigHeroPosition\",\n",
    "\t\"features\": [{\n",
    "\t\t\"size\": \"large\",\n",
    "\t\t\"position\": \"left\"\n",
    "\t}],\n",
    "\t\"baselineAction\": \"Red-Polo-Shirt-432\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_slot_features(rankjsonobj):\n",
    "    rankjsonobj['slots'] = slotfeaturesobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multislot Rank API's prediction with known user preference\n",
    "\n",
    "This function is called after the Multislot Rank API is called, for each iteration.\n",
    "\n",
    "This function compares the user's preferences for product, based on day of month, time of day and device, with the Personalizer's suggestion for the user for those filters. If the suggestion matches, a score of 1 is returned, otherwise the score is 0. The cell has no output. The function does output the score when called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_from_simulated_data(name, dayofmonth, timeofday, device, slotId, prediction):\n",
    "    if(prediction in str(simulated_users[name][dayofmonth][timeofday][device][slotId])):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the calls to Multislot Rank and Reward\n",
    "\n",
    "The next cell is the _main_ work of the Notebook, getting a random user, getting the product list, getting the slot list, sending both to the Multislot Rank API. Comparing the prediction with the user's known preferences, then sending the reward back to the Personalizer service. \n",
    "\n",
    "The loop runs for the `num_requests` times. Personalizer needs a few thousand calls to Multislot Rank and Reward to create a model. \n",
    "\n",
    "An example of the JSON sent to the Multislot Rank API follows. The list of product is not complete, for brevity. You can see the entire JSON for product in `products.json`.\n",
    "\n",
    "```json\n",
    "{\n",
    "\t'contextFeatures': [\n",
    "\t\t{\n",
    "\t\t\t'name': 'Bob', \n",
    "\t\t\t'dayofmonth': '21', \n",
    "\t\t\t'timeofday': 'Afternoon', \n",
    "\t\t\t'device': 'Mobile'\n",
    "\t\t}\n",
    "    ], \n",
    "\t'actions': [\n",
    "\t\t{\n",
    "\t\t\t'id': 'Red-Polo-Shirt-432', \n",
    "\t\t\t'features': [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t'onSale': True, \n",
    "\t\t\t\t\t'price': 20, \n",
    "\t\t\t\t\t'category': 'Clothing'\n",
    "\t\t\t\t}\n",
    "\t\t\t]\n",
    "\t\t}\n",
    "\t\t...rest of product list\n",
    "\t], \n",
    "\t'slots': [\n",
    "\t\t{\n",
    "\t\t\t'id': 'BigHeroPosition', \n",
    "\t\t\t'features': [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t'size': 'large', \n",
    "\t\t\t\t\t'position': 'left'\n",
    "\t\t\t\t}\n",
    "\t\t\t], \n",
    "\t\t\t'baselineAction': 'Red-Polo-Shirt-432'\n",
    "\t\t}\n",
    "\t\t...rest of slot list\n",
    "\t], \t\n",
    "\t'eventId': '3a4d7635ebff45f3b32cff81b119010b', \n",
    "\t'deferActivation': False\n",
    "}\n",
    "```\n",
    "\n",
    "Json sent to the Multislot Rank API:\n",
    "```console\n",
    "{'contextFeatures': [{'name': 'Bob', 'dayofmonth': '21', 'timeofday': 'Afternoon', 'device': 'Mobile'}], 'actions': [{'id': 'Red-Polo-Shirt-432', 'features': [{'onSale': True, 'price': 20, 'category': 'Clothing'}]}, {'id': 'Tennis-Racket-133', 'features': [{'onSale': False, 'price': 70, 'category': 'Sports'}]}, {'id': '31-Inch-Monitor-771', 'features': [{'onSale': True, 'price': 200, 'category': 'Electronics'}]}, {'id': 'XBox-Series X-117', 'features': [{'onSale': False, 'price': 499, 'category': 'Electronics'}]}], 'slots': [{'id': 'BigHeroPosition', 'features': [{'size': 'large', 'position': 'left'}], 'baselineAction': 'Red-Polo-Shirt-432'}, {'id': 'SmallSidebar', 'features': [{'size': 'small', 'position': 'right'}], 'baselineAction': 'Tennis-Racket-133'}], 'excludedActions': [], 'eventId': '3a4d7635ebff45f3b32cff81b119010b', 'deferActivation': False}\n",
    "```\n",
    "\n",
    "Json response from the Multislot Rank API:\n",
    "```console\n",
    "{'slots': [{'id': 'BigHeroPosition', 'rewardActionId': 'Tennis-Racket-133'}, {'id': 'SmallSidebar', 'rewardActionId': 'Red-Polo-Shirt-432'}], 'eventId': '3a4d7635ebff45f3b32cff81b119010b'}\n",
    "```\n",
    "\n",
    "Finally, each loop shows the random selection of user, day of month, time of day, device, slot id and determined reward. The reward of 1 indicates Personalizer resource selected the correct product type for the given user, day of month, time of day, and device.\n",
    "\n",
    "```console\n",
    "1 Alice 30 Morning Mobile BigHeroPosition Red-Polo-Shirt-432 0\n",
    "```\n",
    "\n",
    "The function uses:\n",
    "\n",
    "* Multislot Rank: a POST REST API to get rank \n",
    "* Multislot Reward: a POST REST API to report reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterations(n, modelCheck, jsonFormat, pause_no_events):\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    # default reward value - assumes failed prediction\n",
    "    reward = 0\n",
    "    \n",
    "    num_aggregate_events = 500\n",
    "\n",
    "    # Print out dateTime\n",
    "    #currentDateTime()\n",
    "\n",
    "    # collect results to aggregate in graph\n",
    "    sub_rewards = 0\n",
    "    total_rewards = []\n",
    "    \n",
    "    num_of_slots = len(slotfeaturesobj)\n",
    "    total_slot_rewards = [[0 for x in range((int)(n/num_aggregate_events))] for y in range(num_of_slots)]\n",
    "    \n",
    "    count = []\n",
    "\n",
    "    # default list of user, day of month, time of day, device\n",
    "    namesopt = ['Alice', 'Bob', 'Cathy']\n",
    "    dayofmonthopt = ['7', '21', '30']\n",
    "    timeofdayopt = ['Morning', 'Afternoon', 'Evening']\n",
    "    deviceopt = ['Mobile', 'Desktop']\n",
    "\n",
    "    bin = 0\n",
    "    while(i <= n):        \n",
    "\n",
    "        # create unique id to associate with an event\n",
    "        eventid = add_event_id(jsonFormat)\n",
    "\n",
    "        # generate a random sample\n",
    "        [name, dayofmonth, timeofday, device] = add_random_user_and_contextfeatures(namesopt, dayofmonthopt, timeofdayopt, deviceopt, jsonFormat)\n",
    "\n",
    "        # add action features to rank\n",
    "        add_action_features(jsonFormat)\n",
    "\n",
    "        # add slot features to rank\n",
    "        add_slot_features(jsonFormat)\n",
    "\n",
    "        # show JSON to send to Rank\n",
    "        #print('To: ', jsonFormat)\n",
    "\n",
    "        # choose an action - get prediction from Personalizer\n",
    "        response = requests.post(personalization_rank_url, headers = headers, params = None, json = jsonFormat)\n",
    "\n",
    "        # show Rank prediction\n",
    "        #print('From: ', response.json())\n",
    "\n",
    "        slot_response = response.json()['slots']\n",
    "        \n",
    "        slot_index = 0\n",
    "        for slot in slot_response:\n",
    "\n",
    "            slotId = slot[\"id\"]\n",
    "            prediction = slot[\"rewardActionId\"]\n",
    "            \n",
    "            # compare personalization service recommendation with the simulated data to generate a reward value            \n",
    "            reward = get_reward_from_simulated_data(name, dayofmonth, timeofday, device, slotId, prediction)\n",
    "\n",
    "            # show result for iteration            \n",
    "            print(f'{i} {name} {dayofmonth} {timeofday} {device} {slotId} {prediction} {reward} ')\n",
    "\n",
    "            # send the reward to the service\n",
    "            response = requests.post(personalization_reward_url + eventid + \"/reward\", headers = headers, params= None, json = { \"reward\": [{ \"slotId\": slotId, \"value\": reward }]})\n",
    "\n",
    "            # for every N rank requests, compute total reward for all slots\n",
    "            sub_rewards = sub_rewards + reward                        \n",
    "            \n",
    "            # for every N rank response, compute reward per slot\n",
    "            total_slot_rewards[slot_index][bin] = total_slot_rewards[slot_index][bin] + reward\n",
    "            \n",
    "            slot_index = slot_index + 1\n",
    "            \n",
    "        # every N iteration, get last updated model date and time\n",
    "        if(i % modelCheck == 0):\n",
    "\n",
    "            print(\"**** 10% of loop found\")\n",
    "            get_last_updated(modelLastModified)\n",
    "\n",
    "        # aggregate so chart is easier to read\n",
    "        if(i % num_aggregate_events == 0):            \n",
    "            print(\"**** aggregating rewards\")\n",
    "            total_rewards.append(sub_rewards)            \n",
    "            sub_rewards = 0                         \n",
    "            count.append(i)\n",
    "            \n",
    "            bin = bin + 1\n",
    "        \n",
    "        if(i % pause_no_events == 0):\n",
    "            time.sleep(30)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Print out dateTime\n",
    "    #currentDateTime()   \n",
    "    \n",
    "    return [count, total_rewards, total_slot_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for 25,000 iterations\n",
    "Run the Personalizer loop for 25,000 iterations. This is long running event. Do not close the browser running the notebook. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service. When you have around 50,000 calls, a rank and reward call for each iteration of the loop, the iterations are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max iterations\n",
    "num_requests = 25000\n",
    "\n",
    "# check last mod date N% of time - currently 10%\n",
    "lastModCheck = int(num_requests * .10)\n",
    "\n",
    "jsonTemplate = rankactionsjsonobj\n",
    "\n",
    "# main iterations\n",
    "[count, total_rewards, total_slot_rewards] = iterations(num_requests, lastModCheck, jsonTemplate, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart results to see improvement with Personalizer \n",
    "\n",
    "Create a chart from the count, total_rewards and total_slot_rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChart(x, y, z):\n",
    "    plt.plot(x, y, label = 'reward for all slots', linestyle=\"-\")\n",
    "    for i in range(len(z)):\n",
    "        plt.plot(x, z[i], label = f'reward for slot {i}', linestyle=\"--\")\n",
    "    plt.xlabel(\"Batch of rank events\")\n",
    "    plt.ylabel(\"Correct recommendations per batch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chart for 25,000 rank requests\n",
    "\n",
    "Run the createChart function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createChart(count, total_rewards, total_slot_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the chart\n",
    "\n",
    "This chart shows the success of the model for the current default learning policy. \n",
    "\n",
    "The ideal target that by the end of the test, the loop is averaging a success rate that is close to one hundred percent minus the exploration. The default setting of exploration is 20%. \n",
    "\n",
    "`100-20=80`\n",
    "\n",
    "This exploration setting is found in the Azure portal, for the Personalizer resource, in the Configurations tab, under the RESOURCE MANAGEMENT section \n",
    "\n",
    "In order to find a better learning policy, based on your data to the Rank API, run an [offline evaluation](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-offline-evaluation) in the portal for your Personalizer loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an offline evaluation\n",
    "\n",
    "1. In the Azure portal, open the Personalizer resource's **Evaluations** page.\n",
    "1. Select **Create Evaluation**.\n",
    "1. Enter the required data of evaluation name, and date range for the loop evaluation. The date range should include only the days you are focusing on for your evaluation. \n",
    "\n",
    "    The purpose of running this offline evaluation is to determine if there is a better learning policy for the features and actions used in this loop. To find that better learning policy, make sure **Optimization policy** is turned on.\n",
    "\n",
    "1. Select **OK** to begin the evaluation. \n",
    "1. This **Evaluations** page lists the new evaluation and its current status. Depending on how much data you have, this evaluation can take some time. You can come back to this page after a few minutes to see the results. \n",
    "1. When the evaluation is completed, select the evaluation then select **Comparison of different learning policies**. This shows the available learning policies and how they would behave with the data. \n",
    "1. Select the top-most learning policy in the table and select **Apply**. This applies the _best_ learning policy to your model and retrains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate new learning policy by running experiment for 5,000 iterations\n",
    "\n",
    "Return to the Azure notebook, and continue by running the same loop but for only 5,000 iterations. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service. When you have around 10,000 calls, a rank and reward call for each iteration of the loop, the iterations are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max iterations\n",
    "num_requests = 5000\n",
    "\n",
    "# check last mod date N% of time - currently 10%\n",
    "lastModCheck2 = int(num_requests * .10)\n",
    "\n",
    "jsonTemplate2 = rankactionsjsonobj\n",
    "\n",
    "# main iterations\n",
    "[count2, total_rewards2, total_slot_rewards2] = iterations(num_requests, lastModCheck2, jsonTemplate2, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chart for 5,000 rank requests\n",
    "\n",
    "Run the `createChart` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createChart(count2, total_rewards2, total_slot_rewards2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the second chart\n",
    "\n",
    "The second chart should show a visible increase in Rank predictions aligning with user preferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "If you do not intend to continue the tutorial series, clean up the following resources:\n",
    "\n",
    "* Delete your Azure Notebook project. \n",
    "* Delete your Personalizer resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
